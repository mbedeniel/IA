üßä Apprentissage par Renforcement : R√©solution de Frozen Lake

Bienvenue dans ce d√©p√¥t ! Ce projet pr√©sente l'impl√©mentation compl√®te et l'analyse de trois algorithmes fondamentaux d'Apprentissage par Renforcement (Reinforcement Learning) appliqu√©s √† un environnement personnalis√© inspir√© du c√©l√®bre jeu Frozen Lake.
üéÆ Qu'est-ce que Frozen Lake ?

![D√©mo Frozen Lake](frozen_lake.gif)

Le "Frozen Lake" (Lac Gel√©) est un probl√®me classique de navigation sur grille (Gridworld). L'agent se d√©place sur une surface glissante et doit atteindre un objectif sans tomber dans les trous d'eau glac√©e.

Dans ce projet, l'environnement a √©t√© enti√®rement mod√©lis√© sur mesure :

    La Grille : Un plateau de 7x7 (49 √©tats distincts de S00 √† S66).

    D√©part et Arriv√©e : L'agent commence en bas √† droite (S66) et doit atteindre le but en haut √† gauche (S00).

    Les Dangers : 7 trous pars√®ment le lac (S06, S22, S40, S42, S45, S55, S63).

    Les Actions : Droite, Gauche, Haut, Bas, et une action sp√©cifique "Arriver" pour valider l'objectif.

    Syst√®me de R√©compenses :

        +10 pour l'atteinte de l'objectif.

        -10 en cas de chute dans un trou.

        0 pour une case de glace standard.

üöÄ Algorithmes Impl√©ment√©s

J'ai d√©velopp√© et compar√© trois approches distinctes pour r√©soudre ce Processus de D√©cision de Markov (MDP) :

    Policy Iteration : Un algorithme Model-Based qui alterne entre l'√©valuation de la politique actuelle et son am√©lioration jusqu'√† convergence.

    Value Iteration : Une approche √©galement Model-Based qui met directement √† jour la valeur des √©tats en utilisant l'op√©rateur d'optimalit√© de Bellman, convergeant plus rapidement que Policy Iteration.

    Q-learning (Œµ-Greedy) : Un algorithme Model-Free qui apprend par l'exp√©rience. L'agent interagit avec l'environnement par essais et erreurs, en g√©rant le compromis Exploration vs Exploitation gr√¢ce au param√®tre epsilon (Œµ).

üìä R√©sultats et Comparaison

Le projet inclut des outils de visualisation cr√©√©s sur mesure (Heatmaps de la grille et graphiques d'√©volution des valeurs d'√©tats) pour analyser la convergence.

Points cl√©s de l'analyse :

    Vitesse de convergence : Value Iteration s'est av√©r√© le plus rapide (environ 7 it√©rations pour 0.5s d'ex√©cution) compar√© √† Policy Iteration (~16 it√©rations, 4s). Cependant, Value Iteration est plus gourmand en m√©moire.

    Le compromis Exploration/Exploitation en Q-Learning :

        Un Œµ faible (0.1) permet une convergence rapide (~16 √©pisodes) mais risque de bloquer l'agent dans une politique sous-optimale (√©tats inexplor√©s).

        Un Œµ mod√©r√© (0.3) offre le meilleur √©quilibre, permettant de trouver un chemin optimal (~13 actions pour le but) en un temps raisonnable (~82 √©pisodes).

        Un Œµ √©lev√© (0.7) favorise une exploration totale de la grille, mais ralentit drastiquement la convergence (Q-table instable car n'exploitant pas ses connaissances).

üí° Comp√©tences Acquises

Ce projet m'a permis de d√©velopper une solide expertise technique et th√©orique :

    Th√©orie de l'Apprentissage par Renforcement : Compr√©hension approfondie des Processus de D√©cision de Markov, de l'√©quation de Bellman, des m√©thodes bas√©es sur les valeurs/politiques, et du dilemme exploration/exploitation.

    Programmation Python Orient√©e Donn√©es : Utilisation intensive de la biblioth√®que Pandas pour mod√©liser l'environnement, les tables de transition (DataFrames de Q-Values, Q-Tables, Politiques), ainsi que Numpy pour les tirages probabilistes al√©atoires.

    Data Visualization : Cr√©ation de fonctions d'affichage complexes avec Matplotlib (Heatmaps avec superposition de texte et de formes g√©om√©triques pour repr√©senter visuellement les √©tats de la grille, et graphiques en escalier pour suivre l'√©volution des algorithmes).

    Analyse d'Algorithmes : Benchmarking du temps d'ex√©cution (via le module time), profilage de la convergence et r√©glage des hyperparam√®tres (Gamma, Alpha, Delta, Epsilon).
